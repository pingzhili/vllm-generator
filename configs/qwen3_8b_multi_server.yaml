# Configuration for multiple vLLM servers with Qwen models

data:
  input_path: "data/large_dataset.parquet"
  output_path: "data/qwen_responses.parquet"
  input_column: "prompt"
  output_column: "completion"
  # Filter specific rows
  filter_condition: "category == 'technical' and difficulty >= 3"
  limit: 10000
  shuffle: true

models:
  # Multiple Qwen model endpoints for load balancing
  - url: "http://gpu-server-1:8000"
    name: "qwen3-8b-server-1"
  - url: "http://gpu-server-2:8000"
    name: "qwen3-8b-server-2"
  - url: "http://gpu-server-3:8000"
    name: "qwen3-8b-server-3"

generation:
  num_samples: 1
  temperature: 0.7
  top_p: 0.9
  top_k: 40
  max_tokens: 2048
  presence_penalty: 0.1
  frequency_penalty: 0.1

processing:
  batch_size: 128
  num_workers: 6  # Distribute across 3 servers with 2 workers each
  checkpoint_interval: 100
  checkpoint_dir: "./checkpoints/qwen_run"
  resume: false

retry:
  max_retries: 3
  retry_delay: 1.0
  timeout: 300
  backoff_factor: 2.0

logging:
  level: "INFO"
  file: "logs/qwen_generation.log"
  format: "{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} | {message}"
  rotation: "1 day"
  retention: "14 days"