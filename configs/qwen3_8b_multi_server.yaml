# Configuration for multi-server parallel generation
model_config:
  model: "Qwen/Qwen3-8B"
  temperature: 0.6
  top_p: 0.95
  max_tokens: 32768
  gpu_memory_utilization: 0.9
  tensor_parallel_size: 1  # Each server uses 1 GPUs

generation_config:
  batch_size: 8
  num_repeats: 16
  repeat_strategy: "independent"
  error_handling: "retry"
  max_retries: 3
  checkpoint_frequency: 25

data_config:
  question_column: "problem"
  output_format: "nested"
  output_column_prefix: "response"
  use_chat_template: true
#  system_prompt: "You are a helpful AI assistant."

parallel_config:
  mode: "multi_server"
  num_workers: 8
  worker_gpus: [0, 1, 2, 3, 4, 5, 6, 7]  # 8 GPUs total, 1 per worker
  sharding_strategy: "balanced"
  ports: 2333
  dynamic_batching: true
  result_aggregation: "sequential"
  worker_failure_mode: "retry"
  max_worker_retries: 5

logging_config:
  level: "INFO"
  save_metadata: true
  track_token_usage: true
  progress_aggregation: true