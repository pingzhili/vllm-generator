# Configuration for use with Ray-based vLLM deployment

data:
  input_path: "data/prompts.parquet"
  output_path: "data/ray_outputs.parquet"
  input_column: "text"
  output_column: "generated_text"
  copy_columns: ["prompt_id", "metadata"]

models:
  # Ray Serve endpoint
  - url: "http://ray-head-node:8000"
    name: "ray_vllm_cluster"
    headers:
      Authorization: "Bearer YOUR_API_KEY"

generation:
  num_samples: 3
  temperature: 0.8
  top_p: 0.95
  max_tokens: 1024
  # Use seed for reproducible results
  seed: 12345

processing:
  # Larger batch size for Ray's efficient batching
  batch_size: 256
  # Single worker since Ray handles distribution
  num_workers: 1
  checkpoint_interval: 200
  checkpoint_dir: "./checkpoints/ray_run"

retry:
  max_retries: 10
  retry_delay: 5.0
  timeout: 1200  # Longer timeout for Ray
  backoff_factor: 1.5

logging:
  level: "DEBUG"
  file: "logs/ray_vllm.log"